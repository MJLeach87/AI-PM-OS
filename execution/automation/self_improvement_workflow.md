# Self-Improvement Workflow Automation

**Component**: Phase 3 Self-Improvement Loop
**Created**: 2026-02-01
**Version**: 1.0
**Status**: Active

---

## Overview

The Self-Improvement Workflow enables PM OS to continuously improve itself through automated quality audits, pattern detection, and improvement proposal generation. This workflow is orchestrated by the System Evaluator agent and runs on a weekly schedule.

**Target**: 70% of improvements agent-generated by Phase 3 completion

---

## Workflow Architecture

### High-Level Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                    Weekly Trigger (Sunday 6pm)                   │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                     Phase 1: Quality Audit                       │
│  - Scan execution/ for artifacts (last 7 days)                  │
│  - Analyze against identity/STANDARDS.md                        │
│  - Generate quality audit report                                │
│  - Calculate quality scores                                     │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Phase 2: Pattern Detection                    │
│  - Identify recurring issues across artifacts                   │
│  - Detect successful patterns                                   │
│  - Calculate pattern frequency and impact                       │
│  - Prioritize issues by NSM impact                              │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│               Phase 3: Improvement Proposal Generation           │
│  - Select top 3-5 issues from quality audit                     │
│  - Generate improvement proposals using template                │
│  - Calculate expected impact on NSMs                            │
│  - Create implementation plans                                  │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                  Phase 4: Pull Request Creation                  │
│  - Create feature branches for each proposal                    │
│  - Commit proposals to execution/improvement_proposals/         │
│  - Generate pull requests with detailed descriptions            │
│  - Tag human PM for review                                      │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                   Phase 5: Human PM Review                       │
│  - Review proposals for strategic alignment                     │
│  - Approve/reject/modify proposals                              │
│  - Prioritize implementation order                              │
│  - Merge approved proposals                                     │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                 Phase 6: Implementation & Validation             │
│  - System Evaluator implements approved changes                 │
│  - Run regression tests                                         │
│  - Generate test artifacts                                      │
│  - Measure quality score improvement                            │
└────────────────────────────┬────────────────────────────────────┘
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                  Phase 7: Performance Dashboard                  │
│  - Update performance metrics                                   │
│  - Generate weekly dashboard                                    │
│  - Track agent-generated improvement percentage                 │
│  - Alert on regressions or critical issues                      │
└─────────────────────────────────────────────────────────────────┘
```

---

## Triggering Mechanisms

### Automatic Triggers

**Weekly Scheduled Audit**:
- **Schedule**: Every Sunday at 6:00 PM
- **Scope**: Last 7 days of artifacts
- **Output**: Quality audit report + performance dashboard
- **Implementation**: Cron job / scheduled task / IDE automation

**Post-Update Validation**:
- **Trigger**: After any agent logic update (merge to main)
- **Scope**: All artifacts generated since update
- **Output**: Regression detection report
- **Purpose**: Ensure updates don't degrade quality

**Threshold-Based Alerts**:
- **Trigger**: Quality score drops below 85/100
- **Trigger**: Rework rate exceeds 15%
- **Trigger**: Sprint readiness falls below 90%
- **Action**: Immediate quality audit + alert human PM

### Manual Triggers

**On-Demand Audit**:
```bash
# Claude Code invocation
"System Evaluator: Run quality audit for [time period / specific agent / artifact type]"
```

**Targeted Analysis**:
```bash
# Focus on specific issue
"System Evaluator: Analyze pattern of [missing accessibility requirements / weak strategic alignment / etc.]"
```

**Agent-Specific Audit**:
```bash
# Audit single agent
"System Evaluator: Audit Product Architect outputs from last 30 days"
```

---

## Phase 1: Quality Audit Automation

### File Discovery

**Step 1: Scan execution/ directory**
```bash
# Find all artifacts modified in last 7 days
find execution/ -name "*.md" -mtime -7 -type f
```

**Step 2: Categorize by type**
- `execution/discovery/*.md` → OSTs, research notes
- `execution/prds/*.md` → PRDs
- `execution/technical_specs/*.md` → Technical specs
- `execution/prototypes/*.md` → Prototypes
- `execution/data_analysis/*.md` → SQL queries, metrics reports
- `execution/gtm/*.md` → GTM materials

**Step 3: Determine agent attribution**
- Parse artifact metadata or commit history
- Identify which agent generated each artifact

### Quality Assessment

**Load Standards**:
- Read `identity/STRATEGY.md` for NSM alignment validation
- Read `identity/STANDARDS.md` for quality standards
- Read relevant template for structure validation

**Automated Checks**:
1. **Strategic Alignment** (via Grep):
   ```bash
   # Check if artifact cites NSM
   grep -i "North Star\|NSM\|Time-to-Spec\|Sprint Readiness\|Discovery\|Traceability" [artifact-path]
   ```

2. **BMAD Structure** (for PRDs):
   ```bash
   # Check for required sections
   grep "## Business Case\|## Metrics\|## Approach\|## Details" [prd-path]
   ```

3. **Security Review** (for Technical Specs):
   ```bash
   # Check for security section
   grep -i "Security\|Authentication\|Authorization\|OWASP" [spec-path]
   ```

4. **Template Adherence**:
   - Compare artifact structure to relevant template
   - Identify missing sections
   - Check for placeholder text not replaced

5. **Evidence-Based Reasoning**:
   ```bash
   # Look for data citations
   grep -E "\[.*\]\(.*\)|\bcites?\b|\bdata shows?\b|\bresearch indicates?\b" [artifact-path]
   ```

**Manual Review Points**:
- Professional writing quality
- Logical coherence
- Technical accuracy
- Completeness

### Report Generation

**Output**: `execution/improvement_proposals/YYYY-MM-DD_QualityAudit_Week-N.md`

**Structure** (using quality_audit_template.md):
- Executive summary with overall quality score
- Detailed findings by category
- Pattern analysis
- Impact assessment
- Prioritized recommendations

**Quality Score Calculation**:
```
Overall Score = (Strategic Alignment × 0.25) +
                (Standards Compliance × 0.25) +
                (Template Adherence × 0.20) +
                (Evidence-Based Reasoning × 0.15) +
                (Completeness × 0.15)

Where each component is scored 0-100
```

---

## Phase 2: Pattern Detection

### Pattern Identification Methods

**Method 1: Grep-Based Pattern Search**
```bash
# Find artifacts missing accessibility requirements
grep -L "accessibility\|WCAG\|screen reader" execution/technical_specs/*.md

# Count frequency
total_specs=$(ls -1 execution/technical_specs/*.md | wc -l)
missing_a11y=$(grep -L "accessibility" execution/technical_specs/*.md | wc -l)
percentage=$((100 * missing_a11y / total_specs))
```

**Method 2: Section Completeness Analysis**
- Parse each artifact
- Check for presence of required sections
- Identify which sections most commonly missing

**Method 3: Git History Analysis**
```bash
# Find PRDs requiring multiple revision rounds
git log --all --grep="revision\|rework\|clarification" execution/prds/*.md

# Calculate average revision count per artifact
git log --oneline execution/prds/[specific-prd].md | wc -l
```

**Method 4: Cross-Agent Pattern Detection**
- Identify if specific agent consistently has same issue
- Compare agent outputs for common characteristics
- Detect successful patterns (what works well)

### Pattern Categorization

**Recurring Issues** (negative patterns):
1. Pattern name
2. Frequency (X/Y artifacts, Z%)
3. Root cause hypothesis
4. Impact on NSMs
5. Example instances (file paths)

**Successful Patterns** (positive patterns):
1. Pattern name
2. Frequency
3. Why it works
4. Impact on NSMs
5. Example instances

### Pattern Prioritization

**Priority Score** = (Frequency × Impact × Effort⁻¹)

Where:
- **Frequency**: How often does this occur? (0-100)
- **Impact**: How much does this affect NSMs? (0-100)
- **Effort**: How hard to fix? (XS=10, S=20, M=40, L=70, XL=100)

**Top 3-5 patterns** selected for improvement proposals.

---

## Phase 3: Improvement Proposal Generation

### Proposal Creation Workflow

For each high-priority pattern:

**Step 1: Problem Analysis**
- Describe issue in detail
- Provide specific examples with file paths
- Calculate frequency and impact
- Perform root cause analysis (5 Whys)

**Step 2: Solution Design**
- Identify agent logic changes needed
- Specify template modifications
- Design quality gate enhancements
- Consider orchestrator routing updates

**Step 3: Implementation Planning**
- Break down into phases (preparation, implementation, validation, deployment)
- Estimate effort for each phase
- Identify dependencies
- Create rollback plan

**Step 4: Impact Projection**
- Calculate expected improvement in quality score
- Project NSM impact (which metrics improve, by how much)
- Estimate time savings or rework reduction
- Define success criteria

**Step 5: Proposal Writing**
- Use `templates/improvement_proposal_template.md`
- Include before/after code comparisons
- Document alternatives considered
- Specify validation scenarios

**Output**: `execution/improvement_proposals/YYYY-MM-DD_Proposal_[brief-title].md`

### Proposal Validation

Before submitting proposals:
- [ ] Problem clearly defined with evidence
- [ ] Solution addresses root cause (not symptom)
- [ ] Expected impact quantified
- [ ] Implementation plan detailed
- [ ] Risks identified with mitigations
- [ ] Success criteria measurable

---

## Phase 4: Pull Request Creation

### Git Workflow

**For each approved proposal**:

```bash
# Create feature branch
git checkout -b improvement/[brief-title]

# Commit proposal
git add execution/improvement_proposals/YYYY-MM-DD_Proposal_[title].md
git commit -m "Propose: [Brief description]

[Detailed commit message with problem, solution, impact]

Resolves: [Issue number if tracked]
Agent-Generated: Yes
Priority: [Critical/High/Medium/Low]

Co-Authored-By: System Evaluator Agent <noreply@pm-os.ai>"

# Push branch
git push -u origin improvement/[brief-title]

# Create pull request via gh CLI
gh pr create \
  --title "Improvement: [Brief Title]" \
  --body "$(cat execution/improvement_proposals/YYYY-MM-DD_Proposal_[title].md)" \
  --label "self-improvement,agent-generated,phase-3" \
  --assignee [human-pm-github-username]
```

### Pull Request Template

**Title Format**: `Improvement: [Brief Title]`

**Labels**:
- `self-improvement`
- `agent-generated`
- `phase-3`
- Priority: `critical` / `high` / `medium` / `low`
- Agent: `product-architect` / `engineering-partner` / etc.

**Description**:
- Link to full proposal markdown
- Executive summary (problem, solution, impact)
- Expected NSM impact
- Effort estimate
- Validation plan

**Reviewers**: Human PM (required)

---

## Phase 5: Human PM Review

### Review Criteria

Human PM evaluates proposals based on:

1. **Strategic Alignment**:
   - Does this advance PM OS vision?
   - Aligns with current phase goals?
   - Fits roadmap priorities?

2. **Impact vs. Effort**:
   - Is expected benefit worth the effort?
   - Are there higher-priority issues?
   - Should we implement now or defer?

3. **Implementation Quality**:
   - Is solution well-designed?
   - Are risks adequately addressed?
   - Is rollback plan sufficient?

4. **Validation Plan**:
   - Are success criteria measurable?
   - Is validation approach sound?
   - Can we detect if it fails?

### Review Actions

**Approve**:
- Merge proposal PR
- Create implementation task
- Assign to System Evaluator (or manual implementation)

**Request Changes**:
- Comment on specific issues
- System Evaluator revises proposal
- Re-submit for review

**Reject**:
- Close PR with explanation
- Document reason for rejection
- Consider if issue needs different solution

**Defer**:
- Label as `deferred`
- Add to backlog for future phase
- Document why deferred

---

## Phase 6: Implementation & Validation

### Implementation Process

**For approved proposals**:

1. **Create Implementation Branch**:
   ```bash
   git checkout -b implement/[brief-title]
   ```

2. **Make Agent Logic Changes**:
   - Update `.cursor/rules/[agent].mdc` (Cursor version)
   - Update `.claude/agents/[agent].md` (Claude Code version)
   - Ensure both versions stay synchronized

3. **Update Templates** (if needed):
   - Modify relevant template in `templates/`
   - Add new sections or checklists
   - Update examples

4. **Update Orchestrator Routing** (if needed):
   - Add new keywords to routing logic
   - Update file path patterns
   - Modify quality gate enforcement

5. **Update Documentation**:
   - Update agent spec metadata (version, last updated)
   - Add entry to improvement history section
   - Update capability descriptions if changed

6. **Commit Changes**:
   ```bash
   git add [modified-files]
   git commit -m "Implement: [Brief description]

   [Details of changes]

   Resolves: #[PR number]
   Implementation of: execution/improvement_proposals/[proposal-file].md
   Agent-Generated: Yes (System Evaluator)

   Co-Authored-By: System Evaluator Agent <noreply@pm-os.ai>"
   ```

### Validation Testing

**Regression Tests**:
1. Generate test artifacts using each agent
2. Verify existing capabilities still work
3. Check quality scores haven't decreased

**Improvement Verification**:
1. Generate test artifacts targeting improved area
2. Verify new quality gate enforced
3. Measure quality score improvement
4. Compare to success criteria from proposal

**Validation Scenarios** (from proposal):
- Run each validation scenario
- Document results
- Compare expected vs. actual outcomes

**Quality Audit Post-Implementation**:
- Run targeted audit on new artifacts
- Compare to baseline from pre-implementation audit
- Calculate actual improvement percentage

### Validation Report

**Output**: `execution/improvement_proposals/YYYY-MM-DD_Implementation-Validation_[title].md`

**Contents**:
- Implementation summary
- Regression test results
- Improvement verification results
- Actual vs. expected impact comparison
- Quality score before/after
- Recommendation: [Approve merge / Rollback / Revise]

---

## Phase 7: Performance Dashboard

### Dashboard Generation

**Frequency**: Weekly (part of Sunday 6pm automation)

**Data Sources**:
1. **Git History**:
   ```bash
   # Count artifacts per agent per week
   git log --since="1 week ago" --grep="Product Architect" --oneline execution/ | wc -l
   ```

2. **Quality Audits**:
   - Current week's audit results
   - Historical audit scores for trending

3. **Improvement Proposals**:
   - Number generated
   - Acceptance rate
   - Implementation status

4. **NSM Metrics**:
   - Time-to-Spec averages
   - Sprint Readiness percentages
   - Discovery artifact counts
   - Identity Traceability rates

### Dashboard Structure

**Output**: `execution/improvement_proposals/YYYY-MM-DD_Performance-Dashboard.md`

**Using**: `templates/performance_dashboard_template.md`

**Key Sections**:
1. **Executive Summary**: Overall system health
2. **NSM Progress**: Tracking all 8 metrics
3. **Agent-by-Agent Performance**: Quality scores, issues, trends
4. **Cross-Agent Analysis**: Collaboration patterns, handoff efficiency
5. **Quality Trend Analysis**: Score trends, regression detection
6. **Improvement Impact Assessment**: Effectiveness of implemented changes
7. **Alerts & Recommendations**: Critical issues, high-priority actions

### Alert System

**Critical Alerts** (notify human PM immediately):
- Quality score drops below 80/100
- Regression detected (quality decreases after update)
- Sprint Readiness falls below 90%
- Security vulnerability found

**High-Priority Alerts** (include in weekly dashboard):
- Quality score between 80-85
- Rework rate exceeds 15%
- Specific agent quality below 75/100
- Pattern detected in >25% of artifacts

**Medium-Priority Alerts** (track but don't escalate):
- Quality score 85-90
- Minor template adherence issues
- Missing non-critical sections

---

## Performance Metrics

### System Evaluator KPIs

**Target**: 70% of improvements agent-generated by Phase 3 completion

**Tracking Metrics**:

1. **Proposal Generation Rate**:
   - Proposals generated per week
   - Target: 3-5 per week during Phase 3

2. **Proposal Acceptance Rate**:
   - % of proposals approved by human PM
   - Target: >70% acceptance rate

3. **Implementation Success Rate**:
   - % of implemented improvements that achieve expected impact
   - Target: >80% success rate

4. **Quality Improvement Velocity**:
   - Average quality score increase per improvement
   - Target: +3-5 points per improvement

5. **Agent-Generated Improvement Percentage**:
   - (Agent-generated improvements / Total improvements) × 100
   - Target: 70% by Phase 3 completion

6. **Audit Accuracy**:
   - % of issues identified that human PM agrees are real issues
   - Target: >85% accuracy

7. **Pattern Detection Effectiveness**:
   - % of detected patterns that lead to approved proposals
   - Target: >50%

### NSM Impact Tracking

**For each implemented improvement, measure**:
- Impact on Time-to-Spec
- Impact on Sprint Readiness
- Impact on Discovery Velocity
- Impact on Identity Traceability

**Cumulative NSM Impact**:
- Total improvement in each NSM since Phase 3 start
- Attribution: % improvement from agent-generated vs. manual changes

---

## Integration with Agent Architecture

### Orchestrator Integration

**Routing to System Evaluator**:
- File path: `execution/improvement_proposals/**/*.md`
- Keywords: "evaluate agents", "quality audit", "self-improvement", "performance dashboard"
- Workflow trigger: Weekly schedule, post-update, on-demand

**Context Injection**:
- Always load: `identity/STRATEGY.md`, `identity/STANDARDS.md`, `identity/ROADMAP.md`
- Load agent specs for agents being audited
- Load historical audits for trending

### Agent Collaboration

**System Evaluator Dependencies**:
- **Product Architect**: Collaborates on improvement proposal writing
- **All Agents**: Analyzes outputs from all agents
- **Orchestrator**: Invoked by orchestrator for post-workflow audits

**Workflow Patterns**:
```
Weekly Trigger → ORCHESTRATOR → SYSTEM EVALUATOR (Quality Audit)
                                      ↓
                              SYSTEM EVALUATOR (Pattern Detection)
                                      ↓
                              SYSTEM EVALUATOR (Proposal Generation)
                                      ↓
                              Git PR Creation → Human PM Review
                                      ↓
                              SYSTEM EVALUATOR (Implementation)
                                      ↓
                              SYSTEM EVALUATOR (Validation)
                                      ↓
                              SYSTEM EVALUATOR (Dashboard Update)
```

---

## Configuration & Setup

### Environment Setup

**Required Tools**:
- Git CLI (for PR creation, history analysis)
- GitHub CLI (`gh`) for automated PR workflow
- Bash/shell access for grep/find operations
- Text processing tools (awk, sed for data extraction)

**Credentials**:
- GitHub personal access token (for API access)
- Store in `.env` (gitignored):
  ```
  GITHUB_TOKEN=ghp_xxxxxxxxxxxx
  GITHUB_REPO=MJLeach87/AI-PM-OS
  HUMAN_PM_GITHUB=MJLeach87
  ```

### Scheduling Setup

**Option 1: Cron (Linux/Mac)**:
```cron
# Run weekly audit every Sunday at 6pm
0 18 * * 0 /path/to/run_weekly_audit.sh
```

**Option 2: Windows Task Scheduler**:
- Create scheduled task
- Trigger: Weekly, Sunday, 6:00 PM
- Action: Run PowerShell script `run_weekly_audit.ps1`

**Option 3: IDE Automation**:
- Use IDE hooks/plugins for scheduled execution
- Cursor: Custom extension for weekly trigger
- Claude Code: Manual invocation with reminder

**Option 4: GitHub Actions** (recommended):
```yaml
# .github/workflows/weekly-audit.yml
name: Weekly Quality Audit
on:
  schedule:
    - cron: '0 18 * * 0'  # Sunday 6pm UTC
  workflow_dispatch:  # Allow manual trigger

jobs:
  quality-audit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run System Evaluator
        run: |
          # Invoke Claude Code or Cursor to run System Evaluator
          # Generate quality audit report
          # Create improvement proposals
          # Submit PRs
```

---

## Usage Examples

### Example 1: Weekly Automated Audit

**Trigger**: Sunday 6pm (cron/scheduled task)

**Execution**:
```bash
# Claude Code invocation
"System Evaluator: Run weekly quality audit for last 7 days. Generate audit report, detect patterns, create improvement proposals for top 3 issues, and update performance dashboard."
```

**Expected Outputs**:
1. `execution/improvement_proposals/2026-02-08_QualityAudit_Week-6.md`
2. `execution/improvement_proposals/2026-02-08_Proposal_Accessibility-Requirements.md`
3. `execution/improvement_proposals/2026-02-08_Proposal_Baseline-Metrics.md`
4. `execution/improvement_proposals/2026-02-08_Proposal_Strategic-Alignment.md`
5. `execution/improvement_proposals/2026-02-08_Performance-Dashboard.md`
6. Pull requests created for each proposal

---

### Example 2: Post-Agent Update Validation

**Trigger**: After merging PR that updates Engineering Partner agent

**Execution**:
```bash
# Claude Code invocation
"System Evaluator: Run regression test for Engineering Partner agent. Analyze all technical specs generated since v1.2 update. Compare quality scores to pre-update baseline. Alert if quality degraded."
```

**Expected Outputs**:
1. `execution/improvement_proposals/2026-02-05_Regression-Test_Engineering-Partner-v1.2.md`
2. Alert if regression detected
3. Rollback recommendation if quality degraded >5 points

---

### Example 3: Targeted Pattern Analysis

**Trigger**: Human PM notices many PRDs missing baselines

**Execution**:
```bash
# Claude Code invocation
"System Evaluator: Analyze pattern of missing baseline metrics in PRDs. How frequently does this occur? Which agent? Root cause? Generate improvement proposal."
```

**Expected Outputs**:
1. `execution/improvement_proposals/2026-02-03_Pattern-Analysis_Missing-Baselines.md`
2. `execution/improvement_proposals/2026-02-03_Proposal_Baseline-Metrics-Enforcement.md`

---

### Example 4: Agent Comparison Analysis

**Trigger**: On-demand analysis request

**Execution**:
```bash
# Claude Code invocation
"System Evaluator: Compare Product Architect vs. Engineering Partner quality scores over last 30 days. Which performs better? What are relative strengths/weaknesses?"
```

**Expected Outputs**:
1. `execution/improvement_proposals/2026-02-10_Agent-Comparison_PA-vs-EP.md`
2. Recommendations for improving lower-performing agent

---

## Troubleshooting

### Issue: Audit reports empty or incomplete

**Causes**:
- No artifacts found in execution/ for specified time period
- Grep patterns not matching artifact content
- File permissions preventing read access

**Solutions**:
- Verify artifacts exist: `ls -la execution/*/`
- Test grep patterns manually
- Check file permissions

---

### Issue: Pattern detection missing obvious issues

**Causes**:
- Grep patterns too restrictive
- Root cause analysis insufficient
- Pattern prioritization algorithm favoring wrong patterns

**Solutions**:
- Broaden search patterns
- Manually review sample of artifacts
- Adjust prioritization weights

---

### Issue: Improvement proposals rejected repeatedly

**Causes**:
- Not addressing root cause (solving symptoms)
- Expected impact unrealistic
- Implementation plan unclear
- Not aligned with PM OS strategy

**Solutions**:
- Deeper root cause analysis (5 Whys)
- Conservative impact estimates
- More detailed implementation steps
- Explicit NSM alignment justification

---

### Issue: Implemented improvements not achieving expected impact

**Causes**:
- Validation testing insufficient
- Success criteria not measurable
- Root cause misidentified
- Agent logic change incomplete

**Solutions**:
- More rigorous validation scenarios
- Quantifiable success metrics
- Validate root cause before implementing
- Review all agent spec sections for consistency

---

## Maintenance & Evolution

### Weekly Maintenance Tasks

1. Review performance dashboard for trends
2. Validate System Evaluator accuracy (spot check audits)
3. Update pattern detection logic if new patterns emerge
4. Adjust prioritization weights based on outcomes

### Monthly Review

1. Calculate agent-generated improvement percentage
2. Assess progress toward 70% target
3. Identify any systematic biases in proposal generation
4. Update audit criteria if standards change

### Phase 3 Completion Checklist

- [ ] 70% of improvements agent-generated
- [ ] Quality audit automation fully operational
- [ ] Performance dashboard generated weekly
- [ ] Improvement proposal acceptance rate >70%
- [ ] Regression detection functioning
- [ ] All templates validated with real outputs

---

## References

**Related Files**:
- `.claude/agents/system_evaluator.md` - System Evaluator agent spec
- `templates/quality_audit_template.md` - Audit report structure
- `templates/improvement_proposal_template.md` - Proposal structure
- `templates/performance_dashboard_template.md` - Dashboard structure
- `identity/STRATEGY.md` - NSM definitions
- `identity/STANDARDS.md` - Quality standards

**Related Agents**:
- System Evaluator - Executes all workflow phases
- Orchestrator - Routes to System Evaluator, enforces quality gates
- All Agents - Subjects of quality audits

---

**Workflow Status**: Documented (Implementation pending)
**Next Actions**:
1. Set up weekly scheduled trigger
2. Run first manual audit to validate workflow
3. Generate test improvement proposal
4. Validate PR creation automation

**Version**: 1.0
**Last Updated**: 2026-02-01
**Maintained By**: System Evaluator Agent + Human PM
